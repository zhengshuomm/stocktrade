#!/usr/bin/env python3
"""
æ•°æ®åº“æ’å…¥ç¨‹åº - å°†å¼‚å¸¸æ•°æ®æ’å…¥åˆ°PostgreSQLæ•°æ®åº“

ç¨‹åºé€»è¾‘è¯¦è§£ï¼š
================

1. ç¨‹åºå…¥å£å’Œå‚æ•°è§£æ
   - æ”¯æŒä¸¤ä¸ªæ•°æ®æ–‡ä»¶å¤¹ï¼šdata å’Œ priority_data
   - å¯é…ç½®æ•°æ®æ¸…ç†å¤©æ•°ï¼ˆé»˜è®¤7å¤©ï¼‰
   - å¯é€‰æ‹©è·³è¿‡æ•°æ®æ¸…ç†æ­¥éª¤
   - æ”¯æŒè¯¦ç»†æ—¥å¿—æ¨¡å¼

2. æ ¸å¿ƒå¤„ç†æµç¨‹
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ 1. è¿æ¥PostgreSQLæ•°æ®åº“ (Neonäº‘æ•°æ®åº“)                      â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚ 2. å¤„ç†volume_outlieræ•°æ®                                   â”‚
   â”‚    â”œâ”€ æŸ¥æ‰¾æœ€æ–°volume_outlier_*.csvæ–‡ä»¶                      â”‚
   â”‚    â”œâ”€ æ£€æŸ¥processed_filesè¡¨é¿å…é‡å¤å¤„ç†                     â”‚
   â”‚    â”œâ”€ è¯»å–å½“å‰æ–‡ä»¶æ•°æ®                                      â”‚
   â”‚    â”œâ”€ ä¸ä¸Šä¸€ä¸ªæ–‡ä»¶æ¯”è¾ƒæ•°æ®ç›¸ä¼¼æ€§ï¼ˆé¿å…é‡å¤æ’å…¥ç›¸åŒæ•°æ®ï¼‰      â”‚
   â”‚    â”œâ”€ è·å–å‰ä¸€å¤©æœ€åä¸€ä¸ªæ—¶é—´æˆ³çš„è‚¡ç¥¨ä»·æ ¼æ•°æ®ï¼ˆç”¨äºlast_day_close_priceï¼‰â”‚
   â”‚    â”œâ”€ å‡†å¤‡æ•°æ®ï¼ˆæ ¼å¼åŒ–æµ®ç‚¹æ•°ç²¾åº¦ã€å¤„ç†ä¿¡å·ç±»å‹ç­‰ï¼‰            â”‚
   â”‚    â”œâ”€ æ‰¹é‡æ’å…¥åˆ°volume_outlierè¡¨ï¼ˆä½¿ç”¨ON CONFLICTå¤„ç†é‡å¤ï¼‰  â”‚
   â”‚    â””â”€ è®°å½•å¤„ç†ç»“æœåˆ°processed_filesè¡¨                      â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚ 3. å¤„ç†oi_outlieræ•°æ®                                       â”‚
   â”‚    â”œâ”€ æŸ¥æ‰¾æœ€æ–°outlier/*.csvæ–‡ä»¶                            â”‚
   â”‚    â”œâ”€ æ£€æŸ¥processed_filesè¡¨é¿å…é‡å¤å¤„ç†                     â”‚
   â”‚    â”œâ”€ è¯»å–å½“å‰æ–‡ä»¶æ•°æ®                                      â”‚
   â”‚    â”œâ”€ ä¸ä¸Šä¸€ä¸ªæ–‡ä»¶æ¯”è¾ƒæ•°æ®ç›¸ä¼¼æ€§                            â”‚
   â”‚    â”œâ”€ å‡†å¤‡æ•°æ®ï¼ˆæ ¼å¼åŒ–æµ®ç‚¹æ•°ç²¾åº¦ã€å¤„ç†ä¿¡å·ç±»å‹ç­‰ï¼‰            â”‚
   â”‚    â”œâ”€ æ‰¹é‡æ’å…¥åˆ°oi_outlierè¡¨ï¼ˆä½¿ç”¨ON CONFLICTå¤„ç†é‡å¤ï¼‰      â”‚
   â”‚    â””â”€ è®°å½•å¤„ç†ç»“æœåˆ°processed_filesè¡¨                      â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚ 4. æ•°æ®æ¸…ç†ï¼ˆå¯é€‰ï¼‰                                         â”‚
   â”‚    â”œâ”€ æ¸…ç†è¶…è¿‡æŒ‡å®šå¤©æ•°çš„volume_outlieræ•°æ®                  â”‚
   â”‚    â”œâ”€ æ¸…ç†è¶…è¿‡æŒ‡å®šå¤©æ•°çš„oi_outlieræ•°æ®                      â”‚
   â”‚    â””â”€ æ¸…ç†è¶…è¿‡æŒ‡å®šå¤©æ•°çš„processed_filesè®°å½•                 â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

3. å…³é”®ç‰¹æ€§
   - é‡å¤å¤„ç†é˜²æŠ¤ï¼šé€šè¿‡processed_filesè¡¨è®°å½•å·²å¤„ç†æ–‡ä»¶
   - æ•°æ®å»é‡ï¼šæ¯”è¾ƒç›¸é‚»æ–‡ä»¶æ•°æ®ç›¸ä¼¼æ€§ï¼Œè·³è¿‡ç›¸åŒæ•°æ®
   - ä¿¡å·ç±»å‹ç®¡ç†ï¼šè‡ªåŠ¨åˆ›å»ºå’Œç®¡ç†signal_typesè¡¨
   - æ—¶åŒºå¤„ç†ï¼šä½¿ç”¨PSTæ—¶åŒºç»Ÿä¸€æ—¶é—´æ ¼å¼
   - æµ®ç‚¹æ•°ç²¾åº¦ï¼šç»Ÿä¸€æ ¼å¼åŒ–æ•°å€¼ç²¾åº¦é¿å…ç²¾åº¦é—®é¢˜
   - å‰ä¸€å¤©æ”¶ç›˜ä»·ï¼šè‡ªåŠ¨è·å–å‰ä¸€å¤©æœ€åä¸€ä¸ªæ—¶é—´æˆ³çš„è‚¡ç¥¨æ”¶ç›˜ä»·åˆ°last_day_close_priceåˆ—
   - é”™è¯¯å¤„ç†ï¼šå®Œæ•´çš„å¼‚å¸¸å¤„ç†å’Œå›æ»šæœºåˆ¶
   - æ‰¹é‡æ“ä½œï¼šä½¿ç”¨execute_valuesæé«˜æ’å…¥æ•ˆç‡
   - å†²çªå¤„ç†ï¼šä½¿ç”¨ON CONFLICT DO UPDATEå¤„ç†é‡å¤æ•°æ®

4. æ•°æ®åº“è¡¨ç»“æ„
   - volume_outlier: å­˜å‚¨æˆäº¤é‡å¼‚å¸¸æ•°æ®
   - oi_outlier: å­˜å‚¨æŒä»“é‡å¼‚å¸¸æ•°æ®  
   - signal_types: å­˜å‚¨ä¿¡å·ç±»å‹å®šä¹‰
   - processed_files: è®°å½•æ–‡ä»¶å¤„ç†çŠ¶æ€

5. æ–‡ä»¶ç»„ç»‡ç»“æ„
   data/ æˆ– priority_data/
   â”œâ”€â”€ volume_outlier/     # æˆäº¤é‡å¼‚å¸¸CSVæ–‡ä»¶
   â”œâ”€â”€ outlier/            # æŒä»“é‡å¼‚å¸¸CSVæ–‡ä»¶
   â””â”€â”€ å…¶ä»–æ•°æ®æ–‡ä»¶å¤¹...

åŠŸèƒ½ï¼š
1. è¯»å– volume_outlier å’Œ oi_outlier CSVæ–‡ä»¶
2. æ£€æŸ¥ processed_files è¡¨é¿å…é‡å¤å¤„ç†
3. æ‰¹é‡æ’å…¥æ•°æ®åˆ°å¯¹åº”æ•°æ®åº“è¡¨
4. è®°å½•å¤„ç†ç»“æœåˆ° processed_files è¡¨

ä½¿ç”¨æ–¹æ³•ï¼š
python3 program/insert_outliers_to_db.py --folder data
python3 program/insert_outliers_to_db.py --folder priority_data
"""

import os
import sys
import pandas as pd
import psycopg2
from psycopg2.extras import execute_values
import argparse
import logging
from datetime import datetime
import pytz
from pathlib import Path
import hashlib

# æ•°æ®åº“è¿æ¥ä¿¡æ¯
DB_CONFIG = {
    'host': 'ep-raspy-river-af178kn5-pooler.c-2.us-west-2.aws.neon.tech',
    'database': 'neondb',
    'user': 'neondb_owner',
    'password': 'npg_actGluWDr3d1',
    'port': 5432,
    'sslmode': 'require'
}

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('insert_outliers.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

class DatabaseInserter:
    def __init__(self, folder_name):
        self.folder_name = folder_name
        self.conn = None
        self.cursor = None
        self.signal_type_cache = {}  # ç¼“å­˜ä¿¡å·ç±»å‹ID
        self.pst_tz = pytz.timezone('US/Pacific')  # PSTæ—¶åŒº
        
    def connect_db(self):
        """è¿æ¥æ•°æ®åº“"""
        try:
            self.conn = psycopg2.connect(**DB_CONFIG)
            self.cursor = self.conn.cursor()
            logger.info("âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ")
            return True
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            return False
    
    def close_db(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        if self.cursor:
            self.cursor.close()
        if self.conn:
            self.conn.close()
        logger.info("ğŸ”Œ æ•°æ®åº“è¿æ¥å·²å…³é—­")
    
    def get_signal_type_id(self, signal_type_name):
        """è·å–ä¿¡å·ç±»å‹IDï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º"""
        if not signal_type_name or signal_type_name == '':
            return None
            
        # æ£€æŸ¥ç¼“å­˜
        if signal_type_name in self.signal_type_cache:
            return self.signal_type_cache[signal_type_name]
        
        try:
            # æŸ¥è¯¢ç°æœ‰ä¿¡å·ç±»å‹
            query = "SELECT id FROM signal_types WHERE signal_name = %s"
            self.cursor.execute(query, (signal_type_name,))
            result = self.cursor.fetchone()
            
            if result:
                signal_id = result[0]
                self.signal_type_cache[signal_type_name] = signal_id
                return signal_id
            else:
                # åˆ›å»ºæ–°çš„ä¿¡å·ç±»å‹
                insert_query = """
                INSERT INTO signal_types (signal_name, description) 
                VALUES (%s, %s) 
                RETURNING id
                """
                description = f"è‡ªåŠ¨åˆ›å»ºçš„ä¿¡å·ç±»å‹: {signal_type_name}"
                self.cursor.execute(insert_query, (signal_type_name, description))
                signal_id = self.cursor.fetchone()[0]
                self.signal_type_cache[signal_type_name] = signal_id
                logger.info(f"âœ… åˆ›å»ºæ–°ä¿¡å·ç±»å‹: {signal_type_name} (ID: {signal_id})")
                return signal_id
                
        except Exception as e:
            logger.error(f"âŒ è·å–ä¿¡å·ç±»å‹IDå¤±è´¥: {signal_type_name} - {e}")
            return None
    
    def check_file_processed(self, csv_filename, file_type):
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å¤„ç†è¿‡"""
        try:
            query = """
            SELECT id, status FROM processed_files 
            WHERE folder_name = %s AND csv_filename = %s AND file_type = %s
            """
            self.cursor.execute(query, (self.folder_name, csv_filename, file_type))
            result = self.cursor.fetchone()
            
            if result:
                logger.info(f"ğŸ“ æ–‡ä»¶å·²å¤„ç†è¿‡: {csv_filename} (çŠ¶æ€: {result[1]})")
                return True, result[1]
            return False, None
        except Exception as e:
            logger.error(f"âŒ æ£€æŸ¥æ–‡ä»¶çŠ¶æ€å¤±è´¥: {e}")
            return False, None
    
    def get_latest_csv_file(self, subfolder, file_pattern):
        """è·å–æŒ‡å®šå­æ–‡ä»¶å¤¹ä¸­æœ€æ–°çš„CSVæ–‡ä»¶"""
        folder_path = Path(self.folder_name) / subfolder
        if not folder_path.exists():
            logger.warning(f"âš ï¸ æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {folder_path}")
            return None, None, None
        
        csv_files = list(folder_path.glob(file_pattern))
        if not csv_files:
            logger.warning(f"âš ï¸ æœªæ‰¾åˆ°åŒ¹é…çš„CSVæ–‡ä»¶: {folder_path}/{file_pattern}")
            return None, None, None
        
        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œè·å–æœ€æ–°æ–‡ä»¶å’Œä¸Šä¸€ä¸ªæ–‡ä»¶
        sorted_files = sorted(csv_files, key=os.path.getmtime)
        latest_file = sorted_files[-1]
        previous_file = sorted_files[-2] if len(sorted_files) > 1 else None
        
        logger.info(f"ğŸ“„ æ‰¾åˆ°æœ€æ–°æ–‡ä»¶: {latest_file}")
        if previous_file:
            logger.info(f"ğŸ“„ æ‰¾åˆ°ä¸Šä¸€ä¸ªæ–‡ä»¶: {previous_file}")
        
        return latest_file, latest_file.name, previous_file
    
    def read_csv_data(self, file_path):
        """è¯»å–CSVæ–‡ä»¶æ•°æ®"""
        try:
            df = pd.read_csv(file_path)
            logger.info(f"ğŸ“Š è¯»å–CSVæ–‡ä»¶: {file_path.name}, è¡Œæ•°: {len(df)}")
            return df
        except Exception as e:
            logger.error(f"âŒ è¯»å–CSVæ–‡ä»¶å¤±è´¥: {file_path}: {e}")
            return None
    
    def compare_data_similarity(self, current_df, previous_df, file_type):
        """æ¯”è¾ƒå½“å‰æ–‡ä»¶å’Œä¸Šä¸€ä¸ªæ–‡ä»¶çš„æ•°æ®ç›¸ä¼¼æ€§"""
        if current_df is None or previous_df is None:
            logger.info("ğŸ“Š æ— æ³•æ¯”è¾ƒæ•°æ®ï¼šç¼ºå°‘æ–‡ä»¶æ•°æ®")
            return False
        
        try:
            # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©æ¯”è¾ƒçš„åˆ—
            if file_type == 'volume_outlier':
                compare_columns = ['contractSymbol', 'lastPrice_new', 'è‚¡ç¥¨ä»·æ ¼(new)']
            else:  # oi_outlier
                compare_columns = ['contractSymbol', 'lastPrice_new', 'è‚¡ç¥¨ä»·æ ¼(new)']
            
            # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
            missing_cols = [col for col in compare_columns if col not in current_df.columns or col not in previous_df.columns]
            if missing_cols:
                logger.warning(f"âš ï¸ ç¼ºå°‘æ¯”è¾ƒåˆ—: {missing_cols}")
                return False
            
            # åˆ›å»ºæ¯”è¾ƒç”¨çš„æ•°æ®æ¡†
            current_compare = current_df[compare_columns].copy()
            previous_compare = previous_df[compare_columns].copy()
            
            # æŒ‰contractSymbolæ’åº
            current_compare = current_compare.sort_values('contractSymbol').reset_index(drop=True)
            previous_compare = previous_compare.sort_values('contractSymbol').reset_index(drop=True)
            
            # æ£€æŸ¥è¡Œæ•°æ˜¯å¦ç›¸åŒ
            if len(current_compare) != len(previous_compare):
                logger.info(f"ğŸ“Š æ•°æ®è¡Œæ•°ä¸åŒ: å½“å‰={len(current_compare)}, ä¸Šä¸€ä¸ª={len(previous_compare)}")
                return False
            
            # æ£€æŸ¥contractSymbolæ˜¯å¦ç›¸åŒ
            if not current_compare['contractSymbol'].equals(previous_compare['contractSymbol']):
                logger.info("ğŸ“Š contractSymbolåˆ—è¡¨ä¸åŒ")
                return False
            
            # æ¯”è¾ƒæ•°å€¼åˆ—ï¼ˆå…è®¸å°çš„æµ®ç‚¹æ•°è¯¯å·®ï¼‰
            tolerance = 1e-6
            for col in ['lastPrice_new', 'è‚¡ç¥¨ä»·æ ¼(new)']:
                current_col = pd.to_numeric(current_compare[col], errors='coerce')
                previous_col = pd.to_numeric(previous_compare[col], errors='coerce')
                
                # æ£€æŸ¥æ˜¯å¦æœ‰NaNå€¼
                if current_col.isna().any() or previous_col.isna().any():
                    logger.info(f"ğŸ“Š {col}åˆ—åŒ…å«NaNå€¼ï¼Œæ— æ³•æ¯”è¾ƒ")
                    return False
                
                # æ¯”è¾ƒæ•°å€¼å·®å¼‚
                diff = abs(current_col - previous_col)
                max_diff = diff.max()
                
                if max_diff > tolerance:
                    logger.info(f"ğŸ“Š {col}åˆ—æœ€å¤§å·®å¼‚: {max_diff:.6f} > {tolerance}")
                    return False
            
            logger.info("âœ… æ•°æ®å®Œå…¨ç›¸åŒï¼Œè·³è¿‡æ’å…¥")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ¯”è¾ƒæ•°æ®å¤±è´¥: {e}")
            return False
    
    def format_float_precision(self, value, precision=2):
        """æ ¼å¼åŒ–æµ®ç‚¹æ•°ç²¾åº¦"""
        if pd.isna(value) or value is None:
            return None
        try:
            # ä¿ç•™2ä½æœ‰æ•ˆæ•°å­—
            if abs(value) < 1e-10:
                return 0.0
            # è®¡ç®—æœ‰æ•ˆæ•°å­—ä½æ•°
            if abs(value) >= 1:
                return round(value, precision)
            else:
                # å¯¹äºå°äº1çš„æ•°ï¼Œè®¡ç®—éœ€è¦çš„å°æ•°ä½æ•°
                import math
                if value == 0:
                    return 0.0
                decimal_places = precision - int(math.floor(math.log10(abs(value)))) - 1
                return round(value, max(0, decimal_places))
        except:
            return None
    
    def get_previous_day_stock_prices(self, current_file_path):
        """è·å–å‰ä¸€å¤©çš„è‚¡ç¥¨ä»·æ ¼æ•°æ®"""
        try:
            # è·å–å½“å‰æ–‡ä»¶çš„å®Œæ•´æ—¶é—´æˆ³
            current_filename = current_file_path.name
            # ä»æ–‡ä»¶åä¸­æå–æ—¶é—´æˆ³ï¼Œæ ¼å¼å¦‚ï¼švolume_outlier_20251003-1537.csv
            if 'volume_outlier_' in current_filename:
                # æå–æ—¶é—´æˆ³éƒ¨åˆ†ï¼švolume_outlier_20251003-1537.csv -> 20251003-1537
                timestamp_part = current_filename.split('volume_outlier_')[1].replace('.csv', '')
                # è§£ææ—¶é—´æˆ³ï¼š20251003-1537 -> 2025-10-03 15:37
                current_datetime = datetime.strptime(timestamp_part, '%Y%m%d-%H%M')
                
                # è®¡ç®—å‰ä¸€å¤©çš„æ—¶é—´æˆ³
                from datetime import timedelta
                previous_datetime = current_datetime - timedelta(days=1)
                previous_date_str = previous_datetime.strftime('%Y%m%d')
                
                # æŸ¥æ‰¾å‰ä¸€å¤©çš„è‚¡ç¥¨ä»·æ ¼æ–‡ä»¶
                stock_price_folder = Path(self.folder_name) / 'stock_price'
                if not stock_price_folder.exists():
                    logger.warning(f"âš ï¸ è‚¡ç¥¨ä»·æ ¼æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {stock_price_folder}")
                    return None
                
                # æŸ¥æ‰¾å‰ä¸€å¤©çš„è‚¡ç¥¨ä»·æ ¼æ–‡ä»¶ï¼ˆæ ¼å¼ï¼šall-YYYYMMDD-HHMM.csvï¼‰
                previous_files = list(stock_price_folder.glob(f'all-{previous_date_str}-*.csv'))
                if not previous_files:
                    logger.warning(f"âš ï¸ æœªæ‰¾åˆ°å‰ä¸€å¤©çš„è‚¡ç¥¨ä»·æ ¼æ–‡ä»¶: {previous_date_str}")
                    return None
                
                # æ‰¾åˆ°å‰ä¸€å¤©æœ€åä¸€ä¸ªæ—¶é—´æˆ³çš„æ–‡ä»¶
                # æŒ‰æ–‡ä»¶åä¸­çš„æ—¶é—´æˆ³æ’åºï¼Œè·å–æœ€åä¸€ä¸ª
                previous_files_with_time = []
                for file in previous_files:
                    try:
                        # ä»æ–‡ä»¶åæå–æ—¶é—´æˆ³ï¼šall-20251002-1459.csv -> 20251002-1459
                        file_timestamp = file.name.split('all-')[1].replace('.csv', '')
                        file_datetime = datetime.strptime(file_timestamp, '%Y%m%d-%H%M')
                        previous_files_with_time.append((file, file_datetime))
                    except Exception as e:
                        logger.warning(f"âš ï¸ æ— æ³•è§£ææ–‡ä»¶åæ—¶é—´æˆ³: {file.name} - {e}")
                        continue
                
                if not previous_files_with_time:
                    logger.warning(f"âš ï¸ å‰ä¸€å¤©æ²¡æœ‰æœ‰æ•ˆçš„æ—¶é—´æˆ³æ–‡ä»¶: {previous_date_str}")
                    return None
                
                # æŒ‰æ—¶é—´æˆ³æ’åºï¼Œè·å–æœ€åä¸€ä¸ªï¼ˆæœ€æ–°çš„ï¼‰
                previous_files_with_time.sort(key=lambda x: x[1])
                previous_file = previous_files_with_time[-1][0]
                previous_file_time = previous_files_with_time[-1][1]
                
                logger.info(f"ğŸ“„ æ‰¾åˆ°å‰ä¸€å¤©æœ€åä¸€ä¸ªæ—¶é—´æˆ³çš„è‚¡ç¥¨ä»·æ ¼æ–‡ä»¶: {previous_file} (æ—¶é—´: {previous_file_time})")
                
                # è¯»å–å‰ä¸€å¤©çš„è‚¡ç¥¨ä»·æ ¼æ•°æ®
                previous_df = pd.read_csv(previous_file)
                logger.info(f"ğŸ“Š è¯»å–å‰ä¸€å¤©è‚¡ç¥¨ä»·æ ¼æ•°æ®: {len(previous_df)} æ¡è®°å½•")
                
                return previous_df
            else:
                logger.warning(f"âš ï¸ æ— æ³•ä»æ–‡ä»¶åæå–æ—¶é—´æˆ³: {current_filename}")
                return None
                
        except Exception as e:
            logger.error(f"âŒ è·å–å‰ä¸€å¤©è‚¡ç¥¨ä»·æ ¼å¤±è´¥: {e}")
            return None
    
    def prepare_volume_data(self, df, previous_stock_prices=None):
        """å‡†å¤‡volume_outlieræ•°æ®"""
        if df is None or df.empty:
            return []
        
        data_list = []
        # ä½¿ç”¨PSTæ—¶é—´ï¼Œæ ¼å¼åŒ–ä¸ºæ•°æ®åº“å¯æ¥å—çš„æ ¼å¼
        current_time = datetime.now(self.pst_tz).strftime('%Y-%m-%d %H:%M:%S')
        
        # åˆ›å»ºå‰ä¸€å¤©è‚¡ç¥¨ä»·æ ¼çš„æ˜ å°„å­—å…¸
        previous_close_prices = {}
        if previous_stock_prices is not None and not previous_stock_prices.empty:
            for _, stock_row in previous_stock_prices.iterrows():
                symbol = str(stock_row.get('symbol', ''))
                close_price = self.format_float_precision(stock_row.get('Close'))
                if symbol and close_price is not None:
                    previous_close_prices[symbol] = close_price
            logger.info(f"ğŸ“Š åˆ›å»ºå‰ä¸€å¤©æ”¶ç›˜ä»·æ˜ å°„: {len(previous_close_prices)} ä¸ªè‚¡ç¥¨")
        
        for _, row in df.iterrows():
            try:
                signal_type_name = str(row.get('signal_type', ''))
                signal_type_id = self.get_signal_type_id(signal_type_name)
                
                # è·å–å‰ä¸€å¤©æ”¶ç›˜ä»·
                symbol = str(row.get('symbol', ''))
                last_day_close_price = previous_close_prices.get(symbol) if symbol in previous_close_prices else None
                
                data = {
                    'contractSymbol': str(row.get('contractSymbol', '')),
                    'strike': self.format_float_precision(row.get('strike')),
                    'signal_type_id': signal_type_id,
                    'folder_name': self.folder_name,
                    'option_type': str(row.get('option_type', '')),
                    'volume_old': self.format_float_precision(row.get('volume_old')),
                    'volume_new': self.format_float_precision(row.get('volume_new')),
                    'amount_threshold': self.format_float_precision(row.get('amount_threshold')),
                    'amount_to_market_cap': self.format_float_precision(row.get('amount_to_market_cap')),
                    'openInterest_new': self.format_float_precision(row.get('openInterest_new')),
                    'expiry_date': str(row.get('expiry_date', '')),
                    'lastPrice_new': self.format_float_precision(row.get('lastPrice_new')),
                    'lastPrice_old': self.format_float_precision(row.get('lastPrice_old')),
                    'symbol': symbol,
                    'stock_price_new': self.format_float_precision(row.get('è‚¡ç¥¨ä»·æ ¼(new)')),
                    'stock_price_old': self.format_float_precision(row.get('è‚¡ç¥¨ä»·æ ¼(old)')),
                    'stock_price_new_open': self.format_float_precision(row.get('è‚¡ç¥¨ä»·æ ¼(new open)')),
                    'stock_price_new_high': self.format_float_precision(row.get('è‚¡ç¥¨ä»·æ ¼(new high)')),
                    'stock_price_new_low': self.format_float_precision(row.get('è‚¡ç¥¨ä»·æ ¼(new low)')),
                    'last_day_close_price': last_day_close_price,
                    'create_time': current_time
                }
                data_list.append(data)
            except Exception as e:
                logger.warning(f"âš ï¸ å¤„ç†volumeæ•°æ®è¡Œå¤±è´¥: {e}")
                continue
        
        return data_list
    
    def prepare_oi_data(self, df):
        """å‡†å¤‡oi_outlieræ•°æ®"""
        if df is None or df.empty:
            return []
        
        data_list = []
        # ä½¿ç”¨PSTæ—¶é—´ï¼Œæ ¼å¼åŒ–ä¸ºæ•°æ®åº“å¯æ¥å—çš„æ ¼å¼
        current_time = datetime.now(self.pst_tz).strftime('%Y-%m-%d %H:%M:%S')
        
        for _, row in df.iterrows():
            try:
                signal_type_name = str(row.get('signal_type', ''))
                signal_type_id = self.get_signal_type_id(signal_type_name)
                
                data = {
                    'contractSymbol': str(row.get('contractSymbol', '')),
                    'strike': self.format_float_precision(row.get('strike')),
                    'oi_change': self.format_float_precision(row.get('oi_change')),
                    'signal_type_id': signal_type_id,
                    'folder_name': self.folder_name,
                    'option_type': str(row.get('option_type', '')),
                    'openInterest_new': self.format_float_precision(row.get('openInterest_new')),
                    'openInterest_old': self.format_float_precision(row.get('openInterest_old')),
                    'amount_threshold': self.format_float_precision(row.get('amount_threshold')),
                    'amount_to_market_cap': self.format_float_precision(row.get('amount_to_market_cap')),
                    'expiry_date': str(row.get('expiry_date', '')),
                    'lastPrice_new': self.format_float_precision(row.get('lastPrice_new')),
                    'lastPrice_old': self.format_float_precision(row.get('lastPrice_old')),
                    'volume': self.format_float_precision(row.get('volume')),
                    'symbol': str(row.get('symbol', '')),
                    'stock_price_new': self.format_float_precision(row.get('è‚¡ç¥¨ä»·æ ¼(new)')),
                    'stock_price_old': self.format_float_precision(row.get('è‚¡ç¥¨ä»·æ ¼(old)')),
                    'stock_price_new_open': self.format_float_precision(row.get('è‚¡ç¥¨ä»·æ ¼(new open)')),
                    'stock_price_new_high': self.format_float_precision(row.get('è‚¡ç¥¨ä»·æ ¼(new high)')),
                    'stock_price_new_low': self.format_float_precision(row.get('è‚¡ç¥¨ä»·æ ¼(new low)')),
                    'create_time': current_time
                }
                data_list.append(data)
            except Exception as e:
                logger.warning(f"âš ï¸ å¤„ç†OIæ•°æ®è¡Œå¤±è´¥: {e}")
                continue
        
        return data_list
    
    def insert_volume_data(self, data_list):
        """æ’å…¥volume_outlieræ•°æ®"""
        if not data_list:
            logger.info("ğŸ“Š æ²¡æœ‰volumeæ•°æ®éœ€è¦æ’å…¥")
            return 0
        
        try:
            # å‡†å¤‡æ’å…¥æ•°æ®
            columns = [
                'contractSymbol', 'strike', 'signal_type_id', 'folder_name', 'option_type',
                'volume_old', 'volume_new', 'amount_threshold', 'amount_to_market_cap',
                'openInterest_new', 'expiry_date', 'lastPrice_new', 'lastPrice_old',
                'symbol', 'stock_price_new', 'stock_price_old', 'stock_price_new_open',
                'stock_price_new_high', 'stock_price_new_low', 'last_day_close_price', 'create_time'
            ]
            
            values = []
            for data in data_list:
                # å°†create_timeè½¬æ¢ä¸ºPSTæ—¶åŒºçš„timestamp
                create_time_pst = f"{data['create_time']} PST"
                data_copy = data.copy()
                data_copy['create_time'] = create_time_pst
                values.append(tuple(data_copy.get(col) for col in columns))
            
            # æ‰¹é‡æ’å…¥ï¼Œä½¿ç”¨PSTæ—¶åŒº
            insert_query = f"""
            INSERT INTO volume_outlier ({', '.join(columns)})
            VALUES %s
            ON CONFLICT (contractSymbol, folder_name, create_time) 
            DO UPDATE SET
                strike = EXCLUDED.strike,
                signal_type_id = EXCLUDED.signal_type_id,
                option_type = EXCLUDED.option_type,
                volume_old = EXCLUDED.volume_old,
                volume_new = EXCLUDED.volume_new,
                amount_threshold = EXCLUDED.amount_threshold,
                amount_to_market_cap = EXCLUDED.amount_to_market_cap,
                openInterest_new = EXCLUDED.openInterest_new,
                expiry_date = EXCLUDED.expiry_date,
                lastPrice_new = EXCLUDED.lastPrice_new,
                lastPrice_old = EXCLUDED.lastPrice_old,
                symbol = EXCLUDED.symbol,
                stock_price_new = EXCLUDED.stock_price_new,
                stock_price_old = EXCLUDED.stock_price_old,
                stock_price_new_open = EXCLUDED.stock_price_new_open,
                stock_price_new_high = EXCLUDED.stock_price_new_high,
                stock_price_new_low = EXCLUDED.stock_price_new_low,
                last_day_close_price = EXCLUDED.last_day_close_price
            """
            
            execute_values(self.cursor, insert_query, values)
            self.conn.commit()
            
            logger.info(f"âœ… æˆåŠŸæ’å…¥ {len(data_list)} æ¡volumeæ•°æ®")
            return len(data_list)
            
        except Exception as e:
            logger.error(f"âŒ æ’å…¥volumeæ•°æ®å¤±è´¥: {e}")
            self.conn.rollback()
            return 0
    
    def insert_oi_data(self, data_list):
        """æ’å…¥oi_outlieræ•°æ®"""
        if not data_list:
            logger.info("ğŸ“Š æ²¡æœ‰OIæ•°æ®éœ€è¦æ’å…¥")
            return 0
        
        try:
            # å‡†å¤‡æ’å…¥æ•°æ®
            columns = [
                'contractSymbol', 'strike', 'oi_change', 'signal_type_id', 'folder_name',
                'option_type', 'openInterest_new', 'openInterest_old', 'amount_threshold',
                'amount_to_market_cap', 'expiry_date', 'lastPrice_new', 'lastPrice_old',
                'volume', 'symbol', 'stock_price_new', 'stock_price_old', 'stock_price_new_open',
                'stock_price_new_high', 'stock_price_new_low', 'create_time'
            ]
            
            values = []
            for data in data_list:
                # å°†create_timeè½¬æ¢ä¸ºPSTæ—¶åŒºçš„timestamp
                create_time_pst = f"{data['create_time']} PST"
                data_copy = data.copy()
                data_copy['create_time'] = create_time_pst
                values.append(tuple(data_copy.get(col) for col in columns))
            
            # æ‰¹é‡æ’å…¥ï¼Œä½¿ç”¨PSTæ—¶åŒº
            insert_query = f"""
            INSERT INTO oi_outlier ({', '.join(columns)})
            VALUES %s
            ON CONFLICT (contractSymbol, folder_name, create_time) 
            DO UPDATE SET
                strike = EXCLUDED.strike,
                oi_change = EXCLUDED.oi_change,
                signal_type_id = EXCLUDED.signal_type_id,
                option_type = EXCLUDED.option_type,
                openInterest_new = EXCLUDED.openInterest_new,
                openInterest_old = EXCLUDED.openInterest_old,
                amount_threshold = EXCLUDED.amount_threshold,
                amount_to_market_cap = EXCLUDED.amount_to_market_cap,
                expiry_date = EXCLUDED.expiry_date,
                lastPrice_new = EXCLUDED.lastPrice_new,
                lastPrice_old = EXCLUDED.lastPrice_old,
                volume = EXCLUDED.volume,
                symbol = EXCLUDED.symbol,
                stock_price_new = EXCLUDED.stock_price_new,
                stock_price_old = EXCLUDED.stock_price_old,
                stock_price_new_open = EXCLUDED.stock_price_new_open,
                stock_price_new_high = EXCLUDED.stock_price_new_high,
                stock_price_new_low = EXCLUDED.stock_price_new_low
            """
            
            execute_values(self.cursor, insert_query, values)
            self.conn.commit()
            
            logger.info(f"âœ… æˆåŠŸæ’å…¥ {len(data_list)} æ¡OIæ•°æ®")
            return len(data_list)
            
        except Exception as e:
            logger.error(f"âŒ æ’å…¥OIæ•°æ®å¤±è´¥: {e}")
            self.conn.rollback()
            return 0
    
    def record_processed_file(self, csv_filename, file_type, file_size, row_count, status='success'):
        """è®°å½•å·²å¤„ç†çš„æ–‡ä»¶"""
        try:
            insert_query = """
            INSERT INTO processed_files (folder_name, csv_filename, file_type, file_size, row_count, status)
            VALUES (%s, %s, %s, %s, %s, %s)
            ON CONFLICT (folder_name, csv_filename) 
            DO UPDATE SET
                file_type = EXCLUDED.file_type,
                file_size = EXCLUDED.file_size,
                row_count = EXCLUDED.row_count,
                status = EXCLUDED.status,
                processed_time = CURRENT_TIMESTAMP
            """
            
            self.cursor.execute(insert_query, (self.folder_name, csv_filename, file_type, file_size, row_count, status))
            self.conn.commit()
            
            logger.info(f"ğŸ“ è®°å½•å¤„ç†æ–‡ä»¶: {csv_filename} (çŠ¶æ€: {status})")
            
        except Exception as e:
            logger.error(f"âŒ è®°å½•å¤„ç†æ–‡ä»¶å¤±è´¥: {e}")
            self.conn.rollback()
    
    def cleanup_old_data(self, days=7):
        """æ¸…ç†è¶…è¿‡æŒ‡å®šå¤©æ•°çš„æ—§æ•°æ®"""
        try:
            logger.info(f"ğŸ§¹ å¼€å§‹æ¸…ç†è¶…è¿‡{days}å¤©çš„æ—§æ•°æ®...")
            
            # æ¸…ç†volume_outlierè¡¨
            volume_query = "DELETE FROM volume_outlier WHERE create_time < NOW() - INTERVAL '%s days'"
            self.cursor.execute(volume_query, (days,))
            volume_deleted = self.cursor.rowcount
            
            # æ¸…ç†oi_outlierè¡¨
            oi_query = "DELETE FROM oi_outlier WHERE create_time < NOW() - INTERVAL '%s days'"
            self.cursor.execute(oi_query, (days,))
            oi_deleted = self.cursor.rowcount
            
            # æ¸…ç†processed_filesè¡¨
            processed_query = "DELETE FROM processed_files WHERE processed_time < NOW() - INTERVAL '%s days'"
            self.cursor.execute(processed_query, (days,))
            processed_deleted = self.cursor.rowcount
            
            # æäº¤æ¸…ç†æ“ä½œ
            self.conn.commit()
            
            logger.info(f"âœ… æ•°æ®æ¸…ç†å®Œæˆ:")
            logger.info(f"  volume_outlier: åˆ é™¤ {volume_deleted} æ¡è®°å½•")
            logger.info(f"  oi_outlier: åˆ é™¤ {oi_deleted} æ¡è®°å½•")
            logger.info(f"  processed_files: åˆ é™¤ {processed_deleted} æ¡è®°å½•")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ¸…ç†æ—§æ•°æ®å¤±è´¥: {e}")
            self.conn.rollback()
            return False
    
    def process_volume_outlier(self):
        """å¤„ç†volume_outlieræ–‡ä»¶"""
        logger.info("ğŸ”„ å¼€å§‹å¤„ç†volume_outlieræ–‡ä»¶...")
        
        # è·å–æœ€æ–°æ–‡ä»¶å’Œä¸Šä¸€ä¸ªæ–‡ä»¶
        file_path, csv_filename, previous_file = self.get_latest_csv_file('volume_outlier', 'volume_outlier_*.csv')
        if not file_path:
            logger.warning("âš ï¸ æœªæ‰¾åˆ°volume_outlieræ–‡ä»¶")
            return False
        
        # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†
        is_processed, status = self.check_file_processed(csv_filename, 'volume_outlier')
        if is_processed and status == 'success':
            logger.info(f"â­ï¸ è·³è¿‡å·²å¤„ç†çš„æ–‡ä»¶: {csv_filename}")
            return True
        
        # è¯»å–å½“å‰æ–‡ä»¶æ•°æ®
        current_df = self.read_csv_data(file_path)
        if current_df is None:
            self.record_processed_file(csv_filename, 'volume_outlier', 0, 0, 'failed')
            return False
        
        # å¦‚æœæœ‰ä¸Šä¸€ä¸ªæ–‡ä»¶ï¼Œæ¯”è¾ƒæ•°æ®ç›¸ä¼¼æ€§
        if previous_file:
            logger.info("ğŸ“Š æ¯”è¾ƒå½“å‰æ–‡ä»¶ä¸ä¸Šä¸€ä¸ªæ–‡ä»¶çš„æ•°æ®...")
            previous_df = self.read_csv_data(previous_file)
            if self.compare_data_similarity(current_df, previous_df, 'volume_outlier'):
                # æ•°æ®ç›¸åŒï¼Œè®°å½•ä¸ºå·²å¤„ç†ä½†è·³è¿‡æ’å…¥
                self.record_processed_file(csv_filename, 'volume_outlier', file_path.stat().st_size, len(current_df), 'skipped')
                logger.info("â­ï¸ æ•°æ®ä¸ä¸Šä¸€ä¸ªæ–‡ä»¶ç›¸åŒï¼Œè·³è¿‡æ’å…¥")
                return True
        
        # è·å–å‰ä¸€å¤©çš„è‚¡ç¥¨ä»·æ ¼æ•°æ®
        previous_stock_prices = self.get_previous_day_stock_prices(file_path)
        
        # å‡†å¤‡æ•°æ®
        data_list = self.prepare_volume_data(current_df, previous_stock_prices)
        if not data_list:
            logger.warning("âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„volumeæ•°æ®")
            self.record_processed_file(csv_filename, 'volume_outlier', file_path.stat().st_size, 0, 'failed')
            return False
        
        # æ’å…¥æ•°æ®
        inserted_count = self.insert_volume_data(data_list)
        
        # è®°å½•å¤„ç†ç»“æœ
        status = 'success' if inserted_count > 0 else 'failed'
        self.record_processed_file(csv_filename, 'volume_outlier', file_path.stat().st_size, len(data_list), status)
        
        return inserted_count > 0
    
    def process_oi_outlier(self):
        """å¤„ç†oi_outlieræ–‡ä»¶"""
        logger.info("ğŸ”„ å¼€å§‹å¤„ç†oi_outlieræ–‡ä»¶...")
        
        # è·å–æœ€æ–°æ–‡ä»¶å’Œä¸Šä¸€ä¸ªæ–‡ä»¶
        file_path, csv_filename, previous_file = self.get_latest_csv_file('outlier', '*.csv')
        if not file_path:
            logger.warning("âš ï¸ æœªæ‰¾åˆ°oi_outlieræ–‡ä»¶")
            return False
        
        # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†
        is_processed, status = self.check_file_processed(csv_filename, 'oi_outlier')
        if is_processed and status == 'success':
            logger.info(f"â­ï¸ è·³è¿‡å·²å¤„ç†çš„æ–‡ä»¶: {csv_filename}")
            return True
        
        # è¯»å–å½“å‰æ–‡ä»¶æ•°æ®
        current_df = self.read_csv_data(file_path)
        if current_df is None:
            self.record_processed_file(csv_filename, 'oi_outlier', 0, 0, 'failed')
            return False
        
        # å¦‚æœæœ‰ä¸Šä¸€ä¸ªæ–‡ä»¶ï¼Œæ¯”è¾ƒæ•°æ®ç›¸ä¼¼æ€§
        if previous_file:
            logger.info("ğŸ“Š æ¯”è¾ƒå½“å‰æ–‡ä»¶ä¸ä¸Šä¸€ä¸ªæ–‡ä»¶çš„æ•°æ®...")
            previous_df = self.read_csv_data(previous_file)
            if self.compare_data_similarity(current_df, previous_df, 'oi_outlier'):
                # æ•°æ®ç›¸åŒï¼Œè®°å½•ä¸ºå·²å¤„ç†ä½†è·³è¿‡æ’å…¥
                self.record_processed_file(csv_filename, 'oi_outlier', file_path.stat().st_size, len(current_df), 'skipped')
                logger.info("â­ï¸ æ•°æ®ä¸ä¸Šä¸€ä¸ªæ–‡ä»¶ç›¸åŒï¼Œè·³è¿‡æ’å…¥")
                return True
        
        # å‡†å¤‡æ•°æ®
        data_list = self.prepare_oi_data(current_df)
        if not data_list:
            logger.warning("âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„OIæ•°æ®")
            self.record_processed_file(csv_filename, 'oi_outlier', file_path.stat().st_size, 0, 'failed')
            return False
        
        # æ’å…¥æ•°æ®
        inserted_count = self.insert_oi_data(data_list)
        
        # è®°å½•å¤„ç†ç»“æœ
        status = 'success' if inserted_count > 0 else 'failed'
        self.record_processed_file(csv_filename, 'oi_outlier', file_path.stat().st_size, len(data_list), status)
        
        return inserted_count > 0
    
    def run(self, cleanup_days=7, no_cleanup=False):
        """è¿è¡Œä¸»ç¨‹åº"""
        logger.info(f"ğŸš€ å¼€å§‹å¤„ç†æ–‡ä»¶å¤¹: {self.folder_name}")
        
        # è¿æ¥æ•°æ®åº“
        if not self.connect_db():
            return False
        
        try:
            # å¤„ç†volume_outlier
            volume_success = self.process_volume_outlier()
            
            # å¤„ç†oi_outlier
            oi_success = self.process_oi_outlier()
            
            # æ•°æ®å¤„ç†å®Œæˆåæ¸…ç†æ—§æ•°æ®
            if volume_success or oi_success:
                logger.info("âœ… æ•°æ®å¤„ç†å®Œæˆ")
                # æ¸…ç†æ—§æ•°æ®ï¼ˆå¦‚æœæœªç¦ç”¨ï¼‰
                if not no_cleanup:
                    self.cleanup_old_data(days=cleanup_days)
                else:
                    logger.info("â­ï¸ è·³è¿‡æ•°æ®æ¸…ç†æ­¥éª¤")
                return True
            else:
                logger.warning("âš ï¸ æ²¡æœ‰æ•°æ®è¢«å¤„ç†")
                return False
                
        except Exception as e:
            logger.error(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            return False
        finally:
            self.close_db()

def main():
    parser = argparse.ArgumentParser(description='å°†å¼‚å¸¸æ•°æ®æ’å…¥åˆ°PostgreSQLæ•°æ®åº“')
    parser.add_argument('--folder', type=str, required=True, 
                       choices=['data', 'priority_data'],
                       help='æ•°æ®æ–‡ä»¶å¤¹åç§°')
    parser.add_argument('--verbose', '-v', action='store_true',
                       help='æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—')
    parser.add_argument('--cleanup-days', type=int, default=7,
                       help='æ¸…ç†è¶…è¿‡æŒ‡å®šå¤©æ•°çš„æ—§æ•°æ® (é»˜è®¤: 7å¤©)')
    parser.add_argument('--no-cleanup', action='store_true',
                       help='è·³è¿‡æ•°æ®æ¸…ç†æ­¥éª¤')
    
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # æ£€æŸ¥æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.folder):
        logger.error(f"âŒ æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {args.folder}")
        sys.exit(1)
    
    # è¿è¡Œç¨‹åº
    inserter = DatabaseInserter(args.folder)
    success = inserter.run(cleanup_days=args.cleanup_days, no_cleanup=args.no_cleanup)
    
    if success:
        logger.info("ğŸ‰ ç¨‹åºæ‰§è¡ŒæˆåŠŸ")
        sys.exit(0)
    else:
        logger.error("ğŸ’¥ ç¨‹åºæ‰§è¡Œå¤±è´¥")
        sys.exit(1)

if __name__ == "__main__":
    main()
